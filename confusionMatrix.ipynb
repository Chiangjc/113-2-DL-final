{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5059a2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "從分開的 CSV 檔案讀取預測結果:\n",
      "  預測檔案: test_predictions.csv\n",
      "  真實標籤檔案: ground_truth.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_predictions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 437\u001b[0m\n\u001b[1;32m    428\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchiikawa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhachiware\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkurimanju\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomonga\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrakko\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshisa\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musagi\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# # 如果是單一檔案\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# analysis = analyze_predictions_from_csv(\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m#     csv_file='combined_results.csv', \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m \n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# 如果是兩個分開的檔案\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_predictions_from_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_predictions.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrue_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mground_truth.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 381\u001b[0m, in \u001b[0;36manalyze_predictions_from_csv\u001b[0;34m(csv_file, pred_csv, true_csv, class_names)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  預測檔案: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  真實標籤檔案: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrue_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 381\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mload_predictions_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m請提供 csv_file 或 (pred_csv 和 true_csv)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 314\u001b[0m, in \u001b[0;36mload_predictions_from_csv\u001b[0;34m(csv_file, pred_csv, true_csv, class_names)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true, y_pred\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pred_csv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m true_csv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# 方法2: 分開的兩個檔案\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     true_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(true_csv)\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# 確保兩個檔案有相同的 image_name 順序\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo_app/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo_app/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo_app/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo_app/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/yolo_app/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_predictions.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def plot_multilabel_confusion_matrices(y_true, y_pred, class_names, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    為每個類別繪製二元混淆矩陣 (每個類別 vs 其他所有類別)\n",
    "    \"\"\"\n",
    "    # 計算多標籤混淆矩陣\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    n_classes = len(class_names)\n",
    "    cols = 3\n",
    "    rows = (n_classes + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if n_classes > 1 else [axes]\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        cm = mcm[i]\n",
    "        \n",
    "        # 繪製熱力圖\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                   xticklabels=['Not ' + class_name, class_name],\n",
    "                   yticklabels=['Not ' + class_name, class_name])\n",
    "        axes[i].set_title(f'{class_name}\\nConfusion Matrix')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    # 隱藏多餘的子圖\n",
    "    for i in range(n_classes, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mcm\n",
    "\n",
    "\n",
    "def plot_label_cooccurrence_matrix(y_true, y_pred, class_names, plot_type='both'):\n",
    "    \"\"\"\n",
    "    繪製標籤共現矩陣 (顯示哪些標籤經常一起出現)\n",
    "    \"\"\"\n",
    "    if plot_type in ['true', 'both']:\n",
    "        # 真實標籤的共現矩陣\n",
    "        true_cooc = np.dot(y_true.T, y_true)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(true_cooc, annot=True, fmt='.1f', cmap='Greens',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('True Labels Co-occurrence Matrix')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Class')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if plot_type in ['pred', 'both']:\n",
    "        # 預測標籤的共現矩陣\n",
    "        pred_cooc = np.dot(y_pred.T, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(pred_cooc, annot=True, fmt='.1f', cmap='Oranges',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Predicted Labels Co-occurrence Matrix')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Class')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_label_frequency_comparison(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    比較真實標籤和預測標籤的頻率分布\n",
    "    \"\"\"\n",
    "    true_freq = y_true.sum(axis=0)\n",
    "    pred_freq = y_pred.sum(axis=0)\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, true_freq, width, label='True', alpha=0.8, color='skyblue')\n",
    "    bars2 = ax.bar(x + width/2, pred_freq, width, label='Predicted', alpha=0.8, color='lightcoral')\n",
    "    \n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Label Frequency: True vs Predicted')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.legend()\n",
    "    \n",
    "    # 在柱狀圖上顯示數值\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_multilabel_metrics_per_class(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    計算每個類別的詳細指標\n",
    "    \"\"\"\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    metrics_data = []\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        tn, fp, fn, tp = mcm[i].ravel()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'Class': class_name,\n",
    "            'TP': tp,\n",
    "            'TN': tn,\n",
    "            'FP': fp,\n",
    "            'FN': fn,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1,\n",
    "            'Specificity': specificity,\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_subset_accuracy_analysis(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    分析子集準確度 (exact match) 的詳細情況\n",
    "    \"\"\"\n",
    "    # 計算每個樣本的匹配情況\n",
    "    exact_matches = np.all(y_true == y_pred, axis=1)\n",
    "    subset_accuracy = np.mean(exact_matches)\n",
    "    \n",
    "    # 統計不同標籤數量的準確度\n",
    "    true_label_counts = y_true.sum(axis=1)\n",
    "    pred_label_counts = y_pred.sum(axis=1)\n",
    "    \n",
    "    # 按真實標籤數量分組的準確度\n",
    "    unique_counts = np.unique(true_label_counts)\n",
    "    accuracy_by_count = []\n",
    "    \n",
    "    for count in unique_counts:\n",
    "        mask = (true_label_counts == count)\n",
    "        if np.any(mask):\n",
    "            acc = np.mean(exact_matches[mask])\n",
    "            accuracy_by_count.append(acc)\n",
    "        else:\n",
    "            accuracy_by_count.append(0)\n",
    "    \n",
    "    # 繪製圖表\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # 子圖1: 按標籤數量的準確度\n",
    "    ax1.bar(unique_counts, accuracy_by_count, alpha=0.7, color='lightgreen')\n",
    "    ax1.set_xlabel('Number of True Labels')\n",
    "    ax1.set_ylabel('Subset Accuracy')\n",
    "    ax1.set_title('Subset Accuracy by Number of True Labels')\n",
    "    ax1.set_xticks(unique_counts)\n",
    "    \n",
    "    # 在柱狀圖上顯示數值\n",
    "    for i, acc in enumerate(accuracy_by_count):\n",
    "        ax1.text(unique_counts[i], acc + 0.01, f'{acc:.3f}', \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # 子圖2: 標籤數量分布比較\n",
    "    bins = np.arange(0, max(max(true_label_counts), max(pred_label_counts)) + 2) - 0.5\n",
    "    ax2.hist(true_label_counts, bins=bins, alpha=0.6, label='True', color='skyblue')\n",
    "    ax2.hist(pred_label_counts, bins=bins, alpha=0.6, label='Predicted', color='lightcoral')\n",
    "    ax2.set_xlabel('Number of Labels per Sample')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Label Counts')\n",
    "    ax2.legend()\n",
    "    ax2.set_xticks(range(int(max(max(true_label_counts), max(pred_label_counts))) + 1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Overall Subset Accuracy: {subset_accuracy:.3f}\")\n",
    "    return subset_accuracy\n",
    "\n",
    "\n",
    "def comprehensive_multilabel_analysis(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    完整的多標籤分類分析\n",
    "    \"\"\"\n",
    "    print(\"=== 多標籤分類完整分析 ===\\n\")\n",
    "    \n",
    "    # 1. 每個類別的混淆矩陣\n",
    "    print(\"1. 每個類別的二元混淆矩陣:\")\n",
    "    mcm = plot_multilabel_confusion_matrices(y_true, y_pred, class_names)\n",
    "    \n",
    "    # 2. 每個類別的詳細指標\n",
    "    print(\"\\n2. 每個類別的詳細指標:\")\n",
    "    metrics_df = compute_multilabel_metrics_per_class(y_true, y_pred, class_names)\n",
    "    print(metrics_df.round(3))\n",
    "    \n",
    "    # 3. 標籤頻率比較\n",
    "    print(\"\\n3. 標籤頻率比較:\")\n",
    "    plot_label_frequency_comparison(y_true, y_pred, class_names)\n",
    "    \n",
    "    # 4. 標籤共現分析\n",
    "    print(\"\\n4. 標籤共現矩陣:\")\n",
    "    plot_label_cooccurrence_matrix(y_true, y_pred, class_names)\n",
    "    \n",
    "    # 5. 子集準確度分析\n",
    "    print(\"\\n5. 子集준確度分析:\")\n",
    "    subset_acc = plot_subset_accuracy_analysis(y_true, y_pred, class_names)\n",
    "    \n",
    "    return {\n",
    "        'confusion_matrices': mcm,\n",
    "        'metrics_per_class': metrics_df,\n",
    "        'subset_accuracy': subset_acc\n",
    "    }\n",
    "\n",
    "\n",
    "# 修改原評估函數以包含多標籤分析\n",
    "def evaluate_model_with_multilabel_analysis(model, test_loader, class_names, device='cuda', \n",
    "                                          threshold=0.5, show_analysis=True):\n",
    "    \"\"\"\n",
    "    評估模型並進行完整的多標籤分析\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predictions = (probs > threshold).float()\n",
    "\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "\n",
    "    if show_analysis:\n",
    "        # 進行完整的多標籤分析\n",
    "        analysis_results = comprehensive_multilabel_analysis(\n",
    "            all_labels, all_predictions, class_names\n",
    "        )\n",
    "        return all_predictions, all_labels, all_probs, analysis_results\n",
    "    else:\n",
    "        return all_predictions, all_labels, all_probs\n",
    "\n",
    "\n",
    "def load_predictions_from_csv(csv_file=None, pred_csv=None, true_csv=None, class_names=None):\n",
    "    \"\"\"\n",
    "    從 CSV 檔案讀取預測結果和真實標籤\n",
    "    \n",
    "    方法1 - 單一 CSV 檔案格式:\n",
    "    image_name,predicted_labels,true_labels\n",
    "    case_001,0 2,0 2\n",
    "    case_002,1 4 5,1 4\n",
    "    case_003,,0 1 3\n",
    "    \n",
    "    方法2 - 分開的兩個 CSV 檔案:\n",
    "    pred.csv: image_name,labels\n",
    "    true.csv: image_name,labels\n",
    "    \"\"\"\n",
    "    \n",
    "    if csv_file is not None:\n",
    "        # 方法1: 單一檔案包含兩者\n",
    "        df = pd.read_csv(csv_file)\n",
    "        n_samples = len(df)\n",
    "        n_classes = len(class_names)\n",
    "        \n",
    "        y_pred = np.zeros((n_samples, n_classes))\n",
    "        y_true = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            # 處理預測標籤\n",
    "            if pd.notna(row['predicted_labels']) and str(row['predicted_labels']).strip():\n",
    "                pred_indices = [int(x) for x in str(row['predicted_labels']).split()]\n",
    "                y_pred[i, pred_indices] = 1\n",
    "            \n",
    "            # 處理真實標籤\n",
    "            if pd.notna(row['true_labels']) and str(row['true_labels']).strip():\n",
    "                true_indices = [int(x) for x in str(row['true_labels']).split()]\n",
    "                y_true[i, true_indices] = 1\n",
    "        \n",
    "        return y_true, y_pred\n",
    "    \n",
    "    elif pred_csv is not None and true_csv is not None:\n",
    "        # 方法2: 分開的兩個檔案\n",
    "        pred_df = pd.read_csv(pred_csv)\n",
    "        true_df = pd.read_csv(true_csv)\n",
    "        \n",
    "        # 確保兩個檔案有相同的 image_name 順序\n",
    "        pred_df = pred_df.sort_values('image_name').reset_index(drop=True)\n",
    "        true_df = true_df.sort_values('image_name').reset_index(drop=True)\n",
    "        \n",
    "        # 檢查 image_name 是否匹配\n",
    "        if not pred_df['image_name'].equals(true_df['image_name']):\n",
    "            print(\"警告: 預測檔案和真實標籤檔案的 image_name 不完全匹配!\")\n",
    "            # 取交集\n",
    "            common_ids = set(pred_df['image_name']) & set(true_df['image_name'])\n",
    "            pred_df = pred_df[pred_df['image_name'].isin(common_ids)].sort_values('image_name').reset_index(drop=True)\n",
    "            true_df = true_df[true_df['image_name'].isin(common_ids)].sort_values('image_name').reset_index(drop=True)\n",
    "            print(f\"使用共同的 {len(common_ids)} 個案例\")\n",
    "        \n",
    "        n_samples = len(pred_df)\n",
    "        n_classes = len(class_names)\n",
    "        \n",
    "        y_pred = np.zeros((n_samples, n_classes))\n",
    "        y_true = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        # 處理預測標籤\n",
    "        for i, row in pred_df.iterrows():\n",
    "            if pd.notna(row['labels']) and str(row['labels']).strip():\n",
    "                pred_indices = [int(x) for x in str(row['labels']).split()]\n",
    "                y_pred[i, pred_indices] = 1\n",
    "        \n",
    "        # 處理真實標籤\n",
    "        for i, row in true_df.iterrows():\n",
    "            if pd.notna(row['labels']) and str(row['labels']).strip():\n",
    "                true_indices = [int(x) for x in str(row['labels']).split()]\n",
    "                y_true[i, true_indices] = 1\n",
    "        \n",
    "        return y_true, y_pred\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"請提供 csv_file 或 (pred_csv 和 true_csv)\")\n",
    "\n",
    "\n",
    "def load_predictions_from_separate_files(pred_csv, true_csv, class_names):\n",
    "    \"\"\"\n",
    "    從兩個分開的 CSV 檔案讀取預測結果和真實標籤\n",
    "    \n",
    "    兩個檔案格式都是:\n",
    "    image_name,labels\n",
    "    case_001,0 2\n",
    "    case_002,1 4 5\n",
    "    case_003,\n",
    "    \"\"\"\n",
    "    return load_predictions_from_csv(pred_csv=pred_csv, true_csv=true_csv, class_names=class_names)\n",
    "\n",
    "def analyze_predictions_from_csv(csv_file=None, pred_csv=None, true_csv=None, class_names=None):\n",
    "    \"\"\"\n",
    "    從 CSV 檔案進行多標籤分析\n",
    "    \n",
    "    使用方法:\n",
    "    1. 單一檔案: analyze_predictions_from_csv(csv_file='combined.csv', class_names=class_names)\n",
    "    2. 分開檔案: analyze_predictions_from_csv(pred_csv='pred.csv', true_csv='true.csv', class_names=class_names)\n",
    "    \"\"\"\n",
    "    if csv_file is not None:\n",
    "        print(f\"從單一 CSV 檔案讀取預測結果: {csv_file}\")\n",
    "        y_true, y_pred = load_predictions_from_csv(csv_file=csv_file, class_names=class_names)\n",
    "    elif pred_csv is not None and true_csv is not None:\n",
    "        print(f\"從分開的 CSV 檔案讀取預測結果:\")\n",
    "        print(f\"  預測檔案: {pred_csv}\")\n",
    "        print(f\"  真實標籤檔案: {true_csv}\")\n",
    "        y_true, y_pred = load_predictions_from_csv(pred_csv=pred_csv, true_csv=true_csv, class_names=class_names)\n",
    "    else:\n",
    "        raise ValueError(\"請提供 csv_file 或 (pred_csv 和 true_csv)\")\n",
    "    \n",
    "    print(f\"讀取了 {len(y_true)} 個樣本，{len(class_names)} 個類別\")\n",
    "    \n",
    "    # 進行完整分析\n",
    "    return comprehensive_multilabel_analysis(y_true, y_pred, class_names)\n",
    "\n",
    "# 使用範例\n",
    "\"\"\"\n",
    "class_names = ['chiikawa', 'hachiware', 'kurimanju', 'momonga', 'rakko', 'shisa', 'usagi']\n",
    "\n",
    "# 方法1: 從模型直接評估\n",
    "predictions, labels, probs, analysis = evaluate_model_with_multilabel_analysis(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    class_names=class_names,\n",
    "    device='cuda',\n",
    "    threshold=0.5,\n",
    "    show_analysis=True\n",
    ")\n",
    "\n",
    "# 方法2: 從單一 CSV 檔案分析（包含預測和真實標籤）\n",
    "analysis_results = analyze_predictions_from_csv(\n",
    "    csv_file='predictions.csv', \n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# 方法3: 從兩個分開的 CSV 檔案分析\n",
    "analysis_results = analyze_predictions_from_csv(\n",
    "    pred_csv='predictions.csv',\n",
    "    true_csv='ground_truth.csv', \n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "# 方法4: 手動載入後分析\n",
    "# 單一檔案\n",
    "y_true, y_pred = load_predictions_from_csv(csv_file='combined.csv', class_names=class_names)\n",
    "\n",
    "# 或分開的檔案\n",
    "y_true, y_pred = load_predictions_from_csv(pred_csv='pred.csv', true_csv='true.csv', class_names=class_names)\n",
    "\n",
    "# 然後分析\n",
    "analysis_results = comprehensive_multilabel_analysis(y_true, y_pred, class_names)\n",
    "\"\"\"\n",
    "\n",
    "class_names = ['chiikawa', 'hachiware', 'kurimanju', 'momonga', 'rakko', 'shisa', 'usagi']\n",
    "\n",
    "# # 如果是單一檔案\n",
    "# analysis = analyze_predictions_from_csv(\n",
    "#     csv_file='combined_results.csv', \n",
    "#     class_names=class_names\n",
    "# )\n",
    "\n",
    "# 如果是兩個分開的檔案\n",
    "analysis = analyze_predictions_from_csv(\n",
    "    pred_csv='test_predictions.csv',\n",
    "    true_csv='ground_truth.csv', \n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f4ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
