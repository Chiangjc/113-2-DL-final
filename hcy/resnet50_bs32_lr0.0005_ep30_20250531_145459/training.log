wandb: Currently logged in as: hcy43 (hcy43-academia-sinica) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /home/michael920403/May15/yo/wandb/run-20250531_145507-xhjzm3ul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-surf-5
wandb: â­ï¸ View project at https://wandb.ai/hcy43-academia-sinica/resnet
wandb: ğŸš€ View run at https://wandb.ai/hcy43-academia-sinica/resnet/runs/xhjzm3ul
/home/michael920403/anaconda3/envs/yolo_app/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/michael920403/anaconda3/envs/yolo_app/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
ä½¿ç”¨è¨­å‚™: cuda
æ¨¡å‹æ¶æ§‹: resnet50
ä½¿ç”¨é è¨“ç·´æ¬Šé‡: True
æ­£åœ¨è¼‰å…¥è¨“ç·´è³‡æ–™...
Dataset åˆå§‹åŒ–å®Œæˆ:
- åœ–ç‰‡ç›®éŒ„: datasets/images/train
- æ¨™ç±¤ç›®éŒ„: datasets/labels/train
- åœ–ç‰‡æ•¸é‡: 2304
- é¡åˆ¥æ•¸é‡: 7
- é¡åˆ¥åˆ—è¡¨: ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6']
æ­£åœ¨è¼‰å…¥é©—è­‰è³‡æ–™...
Dataset åˆå§‹åŒ–å®Œæˆ:
- åœ–ç‰‡ç›®éŒ„: datasets/images/val
- æ¨™ç±¤ç›®éŒ„: datasets/labels/val
- åœ–ç‰‡æ•¸é‡: 288
- é¡åˆ¥æ•¸é‡: 7
- é¡åˆ¥åˆ—è¡¨: ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6']
æ­£åœ¨è¼‰å…¥æ¸¬è©¦è³‡æ–™...
Dataset åˆå§‹åŒ–å®Œæˆ:
- åœ–ç‰‡ç›®éŒ„: datasets/images/test
- æ¨™ç±¤ç›®éŒ„: datasets/labels/test
- åœ–ç‰‡æ•¸é‡: 288
- é¡åˆ¥æ•¸é‡: 7
- é¡åˆ¥åˆ—è¡¨: ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6']
=== é¡åˆ¥åˆ†å¸ƒåˆ†æ ===

Train é›†åˆ:
ç¸½åœ–ç‰‡æ•¸: 2304
  class_0: 639 (27.7%)
  class_1: 370 (16.1%)
  class_2: 313 (13.6%)
  class_3: 293 (12.7%)
  class_4: 291 (12.6%)
  class_5: 264 (11.5%)
  class_6: 642 (27.9%)

Validation é›†åˆ:
ç¸½åœ–ç‰‡æ•¸: 288
  class_0: 84 (29.2%)
  class_1: 55 (19.1%)
  class_2: 33 (11.5%)
  class_3: 33 (11.5%)
  class_4: 38 (13.2%)
  class_5: 27 (9.4%)
  class_6: 77 (26.7%)

Test é›†åˆ:
ç¸½åœ–ç‰‡æ•¸: 288
  class_0: 84 (29.2%)
  class_1: 46 (16.0%)
  class_2: 41 (14.2%)
  class_3: 43 (14.9%)
  class_4: 40 (13.9%)
  class_5: 24 (8.3%)
  class_6: 85 (29.5%)
æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: resnet50, é è¨“ç·´: True
ç‰¹å¾µç¶­åº¦: 2048, é¡åˆ¥æ•¸: 7

é–‹å§‹è¨“ç·´...
æ¨¡å‹: resnet50
æ‰¹æ¬¡å¤§å°: 32
å­¸ç¿’ç‡: 0.0005
æœ€å¤§è¨“ç·´è¼ªæ•¸: 30
Early stopping è€å¿ƒå€¼: 10

Epoch 1/30
----------
Train Loss: 0.2691
Val Loss:   0.2470
Learning Rate: 0.000500
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 2/30
----------
Train Loss: 0.1765
Val Loss:   0.1086
Learning Rate: 0.000500
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 3/30
----------
Train Loss: 0.1351
Val Loss:   0.1022
Learning Rate: 0.000500
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 4/30
----------
Train Loss: 0.1279
Val Loss:   0.1133
Learning Rate: 0.000500

Epoch 5/30
----------
Train Loss: 0.1090
Val Loss:   0.1273
Learning Rate: 0.000500

Epoch 6/30
----------
Train Loss: 0.0934
Val Loss:   0.0928
Learning Rate: 0.000500
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 7/30
----------
Train Loss: 0.0940
Val Loss:   0.2295
Learning Rate: 0.000500

Epoch 8/30
----------
Train Loss: 0.0857
Val Loss:   0.0814
Learning Rate: 0.000500
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 9/30
----------
Train Loss: 0.0845
Val Loss:   0.0710
Learning Rate: 0.000500
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 10/30
----------
Train Loss: 0.0826
Val Loss:   0.0794
Learning Rate: 0.000500

Epoch 11/30
----------
Train Loss: 0.0781
Val Loss:   0.0499
Learning Rate: 0.000500
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 12/30
----------
Train Loss: 0.0729
Val Loss:   0.0704
Learning Rate: 0.000500

Epoch 13/30
----------
Train Loss: 0.0728
Val Loss:   0.0755
Learning Rate: 0.000500

Epoch 14/30
----------
Train Loss: 0.0662
Val Loss:   0.0536
Learning Rate: 0.000500

Epoch 15/30
----------
Train Loss: 0.0707
Val Loss:   0.0665
Learning Rate: 0.000500

Epoch 16/30
----------
Train Loss: 0.0637
Val Loss:   0.0536
Learning Rate: 0.000500

Epoch 17/30
----------
Train Loss: 0.0644
Val Loss:   0.0511
Learning Rate: 0.000250

Epoch 18/30
----------
Train Loss: 0.0393
Val Loss:   0.0252
Learning Rate: 0.000250
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 19/30
----------
Train Loss: 0.0423
Val Loss:   0.0259
Learning Rate: 0.000250

Epoch 20/30
----------
Train Loss: 0.0382
Val Loss:   0.0506
Learning Rate: 0.000250

Epoch 21/30
----------
Train Loss: 0.0359
Val Loss:   0.0227
Learning Rate: 0.000250
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 22/30
----------
Train Loss: 0.0249
Val Loss:   0.0192
Learning Rate: 0.000250
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 23/30
----------
Train Loss: 0.0242
Val Loss:   0.0241
Learning Rate: 0.000250

Epoch 24/30
----------
Train Loss: 0.0267
Val Loss:   0.0405
Learning Rate: 0.000250

Epoch 25/30
----------
Train Loss: 0.0305
Val Loss:   0.0336
Learning Rate: 0.000250

Epoch 26/30
----------
Train Loss: 0.0296
Val Loss:   0.0198
Learning Rate: 0.000250

Epoch 27/30
----------
Train Loss: 0.0225
Val Loss:   0.0298
Learning Rate: 0.000250

Epoch 28/30
----------
Train Loss: 0.0268
Val Loss:   0.0186
Learning Rate: 0.000250
ä¿å­˜æœ€ä½³æ¨¡å‹

Epoch 29/30
----------
Train Loss: 0.0358
Val Loss:   0.0430
Learning Rate: 0.000250

Epoch 30/30
----------
Train Loss: 0.0288
Val Loss:   0.0202
Learning Rate: 0.000250
è¨“ç·´æ­·å²å·²ä¿å­˜åˆ°: outputs/resnet50_bs32_lr0.0005_ep30_20250531_145459/training_history.json
è¨“ç·´æ­·å²åœ–å·²å­˜è‡³: outputs/resnet50_bs32_lr0.0005_ep30_20250531_145459/training_history.png
è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œè©•ä¼°...
wandb: uploading artifact best_multilabel_model; uploading wandb-summary.json
wandb: uploading artifact best_multilabel_model
wandb: uploading artifact best_multilabel_model; uploading history steps 29-38, summary, console lines 253-281
wandb: uploading artifact best_multilabel_model
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        class_0_f1 â–
wandb: class_0_precision â–
wandb:    class_0_recall â–
wandb:   class_0_support â–
wandb:        class_1_f1 â–
wandb: class_1_precision â–
wandb:    class_1_recall â–
wandb:   class_1_support â–
wandb:        class_2_f1 â–
wandb: class_2_precision â–
wandb:    class_2_recall â–
wandb:   class_2_support â–
wandb:        class_3_f1 â–
wandb: class_3_precision â–
wandb:    class_3_recall â–
wandb:   class_3_support â–
wandb:        class_4_f1 â–
wandb: class_4_precision â–
wandb:    class_4_recall â–
wandb:   class_4_support â–
wandb:        class_5_f1 â–
wandb: class_5_precision â–
wandb:    class_5_recall â–
wandb:   class_5_support â–
wandb:        class_6_f1 â–
wandb: class_6_precision â–
wandb:    class_6_recall â–
wandb:   class_6_support â–
wandb:             epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:     learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          macro_f1 â–
wandb:          micro_f1 â–
wandb:   micro_precision â–
wandb:      micro_recall â–
wandb:        train_loss â–ˆâ–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:          val_loss â–ˆâ–„â–„â–„â–„â–ƒâ–‡â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–â–‚â–
wandb: 
wandb: Run summary:
wandb:        class_0_f1 0.96203
wandb: class_0_precision 0.98701
wandb:    class_0_recall 0.93827
wandb:   class_0_support 81
wandb:        class_1_f1 0.96552
wandb: class_1_precision 1
wandb:    class_1_recall 0.93333
wandb:   class_1_support 45
wandb:        class_2_f1 0.96296
wandb: class_2_precision 0.975
wandb:    class_2_recall 0.95122
wandb:   class_2_support 41
wandb:        class_3_f1 1
wandb: class_3_precision 1
wandb:    class_3_recall 1
wandb:   class_3_support 40
wandb:        class_4_f1 0.95522
wandb: class_4_precision 0.9697
wandb:    class_4_recall 0.94118
wandb:   class_4_support 34
wandb:        class_5_f1 0.91892
wandb: class_5_precision 1
wandb:    class_5_recall 0.85
wandb:   class_5_support 20
wandb:        class_6_f1 0.9697
wandb: class_6_precision 0.97561
wandb:    class_6_recall 0.96386
wandb:   class_6_support 83
wandb:             epoch 30
wandb:     learning_rate 0.00025
wandb:          macro_f1 0.96205
wandb:          micro_f1 0.96593
wandb:   micro_precision 0.98489
wandb:      micro_recall 0.94767
wandb:        train_loss 0.0288
wandb:          val_loss 0.02018
wandb: 
wandb: ğŸš€ View run rural-surf-5 at: https://wandb.ai/hcy43-academia-sinica/resnet/runs/xhjzm3ul
wandb: â­ï¸ View project at: https://wandb.ai/hcy43-academia-sinica/resnet
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20250531_145507-xhjzm3ul/logs

=== æ¨¡å‹è©•ä¼°çµæœ ===
class_0: P=0.987, R=0.938, F1=0.962, Support=81
class_1: P=1.000, R=0.933, F1=0.966, Support=45
class_2: P=0.975, R=0.951, F1=0.963, Support=41
class_3: P=1.000, R=1.000, F1=1.000, Support=40
class_4: P=0.970, R=0.941, F1=0.955, Support=34
class_5: P=1.000, R=0.850, F1=0.919, Support=20
class_6: P=0.976, R=0.964, F1=0.970, Support=83

=== æ•´é«”æ€§èƒ½ ===
Macro F1:      0.962
Micro Precision: 0.985
Micro Recall:    0.948
Micro F1:        0.966
è©•ä¼°çµæœå·²ä¿å­˜åˆ°: outputs/resnet50_bs32_lr0.0005_ep30_20250531_145459/evaluation_results.json

è¨“ç·´èˆ‡è©•ä¼°å…¨éƒ¨å®Œæˆï¼
çµæœä¿å­˜åœ¨: outputs/resnet50_bs32_lr0.0005_ep30_20250531_145459
æœ€ä½³æ¨¡å‹: outputs/resnet50_bs32_lr0.0005_ep30_20250531_145459/best_multilabel_model.pt
è¨“ç·´æ­·å²: outputs/resnet50_bs32_lr0.0005_ep30_20250531_145459/training_history.json
è©•ä¼°çµæœ: outputs/resnet50_bs32_lr0.0005_ep30_20250531_145459/evaluation_results.json
å·²ä¸Šå‚³æœ€ä½³æ¨¡å‹åˆ° W&B artifact
